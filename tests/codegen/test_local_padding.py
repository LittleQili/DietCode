import filecmp
import logging
import os
import numpy as np

logger = logging.getLogger(__name__)

from shared import CUDAContext, dietcode_decor, no_local_padding

from ops.shared.utils import get_time_evaluator_results_rpc_wrapper


@dietcode_decor
def test_local_padding():
    """
    This test case shows the performance improvement brought by local padding.
    We compare the compute throughputs between two schedules, one without local
    padding (baseline) and the other with local padding (DietCode).
    
    We use the dense layer as the workload:
    
        Mathematical Expression: Y = XW^T, X : [B × T, I], W : [H, I], Y : [B ×
        T, H]
    
        for (i = 0; i < B × T; ++i)
          for (j = 0: j < H; ++j)
            for (k = 0; k < I; ++k)
              Y[i, j] += X[i, k] * W[j, k]
    
    where (B, T, I, H) stand for (batch size, sequence length, input size,
    hidden dimension) respectively (namings adopted from the BERT model).
    
    We adopt a sample schedule `dense_128x128x4` for this workload. The
    schedule, as its name suggests, has a tile size of (128, 128, 4) along the
    (i, j, k) dimension respectively.
    
    Furthermore, we deliberately set the value of the sequence length T to 60,
    and the input size I to 770, so that
    
        (B * T = 16 * 60 = 960) % 128 ≠ 0
        
        (I = 770) % 4 ≠ 0

    which is common in the case of handling dynamic-shape workloads.

    Due to the imperfect tiling, predicates have to be injected inside the loop
    body, which can greatly hurt the performance. However, we make the following
    key observations:

    - The schedule generated by the auto-scheduler usually consists of 3 main
      stages, namely
      
      - Fetch: Obtain the input data from the global to the shared memory.
      - Compute: Compute output results using the shared memory variables and
        write to the local registers.
      - Writeback: Write the results of the local registers back to the global
        memory.
    
    - The predicates at the *Compute* stage have the biggest impact on the
      runtime performance, but the good news is they duplicate those at the
      Fetch and the Writeback stage and hence can be safely removed. This is in
      essence padding the compute by the size of the local workspace, hence the
      name *local padding*.

    Our evaluations show that the local padding optimization of DietCode can
    significantly boost the performance of the generated CUDA kernel by more
    than 10× in this case (on a modern NVIDIA RTX 3090 GPU).
    """
    from ops.dense.sample_schedule import dense_128x128x4
    from ops.dense.fixture import Dense, cuBLASDenseFixture


    B, T, I, H = 16, 60, 770, 2304
    TFLOPs = 2 * B * T * I * H / 1e12
    
    wkl_func_args = (B * T, I, H)
    cublas_fixture = cuBLASDenseFixture(*wkl_func_args)

    # temporarily disable local padding
    os.environ['DIETCODE_DO_LOCAL_PADDING'] = '0'
    baseline_perf_results = get_time_evaluator_results_rpc_wrapper(
                                wkl_func=Dense,
                                wkl_func_args=wkl_func_args,
                                sched_func_or_str=dense_128x128x4,
                                fixture=cublas_fixture,
                                print_kernel=True
                            )
    os.environ['DIETCODE_DO_LOCAL_PADDING'] = '1'

    dietcode_perf_results = get_time_evaluator_results_rpc_wrapper(
                                wkl_func=Dense,
                                wkl_func_args=wkl_func_args,
                                sched_func_or_str=dense_128x128x4,
                                fixture=cublas_fixture,
                                print_kernel=True,
                                log_kernel_filename="temp_workspace.log",
                                verify_correctness=True
                            )
    assert filecmp.cmp(os.path.dirname(os.path.realpath(__file__))
                           + "/saved_artifacts/test_local_padding.cu",
                       "temp_workspace.log")

    baseline_tflops = TFLOPs / np.average(baseline_perf_results)
    dietcode_tflops = TFLOPs / np.average(dietcode_perf_results)
    logger.info(f"Baseline vs. DietCode: {baseline_tflops} vs. {dietcode_tflops} (TFLOPS)")

    # Performance Expectation
    #
    # =================================
    # GPU       RTX 3090    RTX 2080 Ti
    # --------  --------    -----------
    # Baseline  ~1          ~0.6
    # DietCode  ~11         ~6
    # =================================
    if CUDAContext.device_name == 'NVIDIA GeForce RTX 3090':
        assert baseline_tflops < 2 and dietcode_tflops > 10
    if CUDAContext.device_name == 'NVIDIA GeForce RTX 2080 Ti':
        assert baseline_tflops < 1 and dietcode_tflops > 5


@dietcode_decor
def test_local_padding_ii():
    """
    This test case shows the necessity of NOT shrinking the local workspace,
    which is required for local padding.

    In TVM, the size of the local workspace (e.g., shared memory or registers)
    will be automatically shrinked if the code generator detects that the
    workspace is not fully utilized by the tensor programs. However, this
    optimization can be NOT desirable.

    For example, consider the batched matrix multiply workload:

        Mathematical Expression: C = AB^T, A : [B, T, H / NH], B : [B, T, H /
        NH], C : [B, T, T]
    
        for (i = 0; i < B; ++i)
          for (j = 0: j < T; ++j)
            for (k = 0; k < T; ++k)
              for (l = 0; l < H / NH; ++l)
                C[i, j, k] += A[i, j, l] * B[i, k, l]

    where NH stands for the number of attention heads (also adopted from the
    BERT model, other parameters are the same as above).

    We adopt a sample schedule `batch_matmul_nt_1x128x128x8` for this workload.
    The schedule has a tile size of (1, 128, 128, 8) along the (i, j, k, l)
    dimension respectively. By the tile sizes, the local workspace (i.e., shared
    memory allocations) for A and B are 1×128×8 and 1×128×8 respectively.
    
    This works fine in the case when T=128, but if T is just a little smaller
    (e.g., T=120), the TVM code generator will find out that all the thread
    blocks are not fully utilizing the shared memory variables and shrink them,
    which prevents local padding from taking place.
    """
    from ops.batch_matmul.sample_schedule import batch_matmul_nt_1x128x128x8
    from ops.batch_matmul.fixture import BatchMatmulNT, cuBLASBatchMatmulNTFixture
    

    B, T, H, NH = 16, 120, 768, 12
    TFLOPs = 2 * B * T * T * H / 1e12
    
    wkl_func_args = (B * NH, T, H // NH, T)
    cublas_fixture = cuBLASBatchMatmulNTFixture(*wkl_func_args)

    print(BatchMatmulNT, cublas_fixture, wkl_func_args, batch_matmul_nt_1x128x128x8)

    # temporarily disable local padding
    with no_local_padding():
      baseline_perf_results = get_time_evaluator_results_rpc_wrapper(
                                  wkl_func=BatchMatmulNT,
                                  wkl_func_args=wkl_func_args,
                                  sched_func_or_str=batch_matmul_nt_1x128x128x8,
                                  fixture=cublas_fixture,
                                  print_kernel=True
                              )

    dietcode_perf_results = get_time_evaluator_results_rpc_wrapper(
                                wkl_func=BatchMatmulNT,
                                wkl_func_args=wkl_func_args,
                                sched_func_or_str=batch_matmul_nt_1x128x128x8,
                                fixture=cublas_fixture,
                                print_kernel=True,
                                log_kernel_filename="temp_workspace.log",
                                verify_correctness=True
                            )

    baseline_tflops = TFLOPs / np.average(baseline_perf_results)
    dietcode_tflops = TFLOPs / np.average(dietcode_perf_results)
    logger.info(f"Baseline vs. DietCode: {baseline_tflops} vs. {dietcode_tflops} (TFLOPS)")
